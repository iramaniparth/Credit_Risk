{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\iramaniparth\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "C:\\Users\\iramaniparth\\Anaconda3\\lib\\site-packages\\sklearn\\grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn import cross_validation, metrics   #Additional scklearn functions\n",
    "from sklearn.grid_search import GridSearchCV   #Perforing grid search\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from scipy.stats import skew, boxcox\n",
    "from math import exp, log\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 12, 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data(X, scaler=None):\n",
    "    if not scaler:\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(X)\n",
    "    X = scaler.transform(X)\n",
    "    return X, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\iramaniparth\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:189: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('amex.csv')\n",
    "train.drop(['Unnamed: 0','mvar16','mvar19','mvar17'],axis=1,inplace=True)\n",
    "train.head()\n",
    "test=pd.read_csv('amextest.csv')\n",
    "test.drop(['Unnamed: 0','mvar16','mvar19','mvar17'],axis=1,inplace=True)\n",
    "train['mvar47'].loc[train['mvar47'] == 'L'] = 0\n",
    "train['mvar47'].loc[train['mvar47'] == 'C'] = 1\n",
    "test['mvar47'].loc[test['mvar47'] == 'L'] = 0\n",
    "test['mvar47'].loc[test['mvar47'] == 'C'] = 1\n",
    "kall = train.iloc[:,1:45]\n",
    "kalltr = kall.dropna()\n",
    "\n",
    "values = kall.values\n",
    "\n",
    "from sklearn.preprocessing import Imputer\n",
    "imputer = Imputer()\n",
    "\n",
    "transformed_values = imputer.fit_transform(values)\n",
    "\n",
    "X = pd.DataFrame(transformed_values)\n",
    "X.columns = kall.columns\n",
    "\n",
    "for index in X.columns:\n",
    "    train[index]=X[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "kall = test.iloc[:,1:45]\n",
    "kalltr = kall.dropna()\n",
    "\n",
    "values = kall.values\n",
    "\n",
    "from sklearn.preprocessing import Imputer\n",
    "imputer = Imputer()\n",
    "\n",
    "transformed_values = imputer.fit_transform(values)\n",
    "\n",
    "X = pd.DataFrame(transformed_values)\n",
    "X.columns = kall.columns\n",
    "\n",
    "for index in X.columns:\n",
    "    test[index]=X[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train.drop(['application_key'], axis=1, inplace=True)\n",
    "train_y = train['default_ind']\n",
    "train.drop(['default_ind'], axis=1, inplace=True)\n",
    "key = test['application_key']\n",
    "test.drop(['application_key'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit on training set only.\n",
    "scaler.fit(train)\n",
    "\n",
    "# Apply transform to both the training set and the test set.\n",
    "train1 = scaler.transform(trains)\n",
    "test1 = scaler.transform(tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Make an instance of the Model\n",
    "pca = PCA(.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=0.95, random_state=None,\n",
       "  svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.fit(train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2 = pca.transform(train1)\n",
    "test2 = pca.transform(test1)\n",
    "\n",
    "trains = train2\n",
    "tests = test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80000, 33)\n"
     ]
    }
   ],
   "source": [
    "train20=np.array(train2)\n",
    "mn=np.mean(train20, axis=0)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(train20, np.array(train_y), random_state=0, test_size = 0.25)\n",
    "print(train20.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 0.814467\n",
      "Accuracy on test set: 0.807700\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gbrt1 = GradientBoostingClassifier(random_state=0,max_depth=4, learning_rate=0.1)\n",
    "gbrt1.fit(X_train, y_train)\n",
    "print(\"Accuracy on training set: {:.6f}\".format(gbrt1.score(X_train, y_train)))\n",
    "print(\"Accuracy on test set: {:.6f}\".format(gbrt1.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test20=np.array(test2)\n",
    "ynew = gbrt1.predict_proba(test20)\n",
    "\n",
    "d=pd.DataFrame()\n",
    "d['application_key']=key\n",
    "d['0']=ynew[:,0]\n",
    "d['1']=ynew[:,1]\n",
    "\n",
    "y=gbrt1.predict(test20)\n",
    "\n",
    "d['pred']=y\n",
    "\n",
    "d['max']=d[['0', '1']].max(axis=1)\n",
    "\n",
    "d['max']=d['max'].apply(lambda x: round(x, 2))\n",
    "\n",
    "d['max2']=d['max']-0.001*d['pred']\n",
    "d = d.sort_values(by = ['max2'], ascending = False)\n",
    "\n",
    "sub1 = d[['application_key', 'pred']]\n",
    "\n",
    "sub1.to_csv('The_miner_league_IITGuwahati_109.csv', header = False, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from skopt import BayesSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "ITERATIONS = 5 # 1000\n",
    "TRAINING_SIZE = 80000*0.75 # 20000000\n",
    "TEST_SIZE = 80000*0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model #1\n",
      "Best ROC-AUC: 0.7971\n",
      "Best params: {'colsample_bylevel': 0.4160029192647807, 'colsample_bytree': 0.7304484857455519, 'gamma': 0.13031389926541354, 'learning_rate': 0.042815319280763466, 'max_delta_step': 13, 'max_depth': 21, 'min_child_weight': 2, 'n_estimators': 87, 'reg_alpha': 5.497557739289786e-07, 'reg_lambda': 0.05936070635912049, 'scale_pos_weight': 0.060830282487222144, 'subsample': 0.13556548021189216}\n",
      "\n",
      "Model #2\n",
      "Best ROC-AUC: 0.8\n",
      "Best params: {'colsample_bylevel': 0.8390144719977516, 'colsample_bytree': 0.8844821246070537, 'gamma': 4.358684608480795e-07, 'learning_rate': 0.7988179462781242, 'max_delta_step': 17, 'max_depth': 3, 'min_child_weight': 1, 'n_estimators': 68, 'reg_alpha': 0.0005266983003701547, 'reg_lambda': 276.5424475574225, 'scale_pos_weight': 0.3016410771843142, 'subsample': 0.9923710598637134}\n",
      "\n",
      "Model #3\n",
      "Best ROC-AUC: 0.8\n",
      "Best params: {'colsample_bylevel': 0.8390144719977516, 'colsample_bytree': 0.8844821246070537, 'gamma': 4.358684608480795e-07, 'learning_rate': 0.7988179462781242, 'max_delta_step': 17, 'max_depth': 3, 'min_child_weight': 1, 'n_estimators': 68, 'reg_alpha': 0.0005266983003701547, 'reg_lambda': 276.5424475574225, 'scale_pos_weight': 0.3016410771843142, 'subsample': 0.9923710598637134}\n",
      "\n",
      "Model #4\n",
      "Best ROC-AUC: 0.8\n",
      "Best params: {'colsample_bylevel': 0.8390144719977516, 'colsample_bytree': 0.8844821246070537, 'gamma': 4.358684608480795e-07, 'learning_rate': 0.7988179462781242, 'max_delta_step': 17, 'max_depth': 3, 'min_child_weight': 1, 'n_estimators': 68, 'reg_alpha': 0.0005266983003701547, 'reg_lambda': 276.5424475574225, 'scale_pos_weight': 0.3016410771843142, 'subsample': 0.9923710598637134}\n",
      "\n",
      "Model #5\n",
      "Best ROC-AUC: 0.8\n",
      "Best params: {'colsample_bylevel': 0.8390144719977516, 'colsample_bytree': 0.8844821246070537, 'gamma': 4.358684608480795e-07, 'learning_rate': 0.7988179462781242, 'max_delta_step': 17, 'max_depth': 3, 'min_child_weight': 1, 'n_estimators': 68, 'reg_alpha': 0.0005266983003701547, 'reg_lambda': 276.5424475574225, 'scale_pos_weight': 0.3016410771843142, 'subsample': 0.9923710598637134}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classifier\n",
    "bayes_cv_tuner = BayesSearchCV(\n",
    "    estimator = xgb.XGBClassifier(\n",
    "        n_jobs = 1,\n",
    "        objective = 'binary:logistic',\n",
    "        eval_metric = 'auc',\n",
    "        silent=1,\n",
    "        tree_method='approx',\n",
    "        early_stopping_rounds=10,\n",
    "    ),\n",
    "    search_spaces = {\n",
    "        'learning_rate': (0.01, 1.0, 'log-uniform'),\n",
    "        'min_child_weight': (0, 10),\n",
    "        'max_depth': (0, 50),\n",
    "        'max_delta_step': (0, 20),\n",
    "        'subsample': (0.01, 1.0, 'uniform'),\n",
    "        'colsample_bytree': (0.01, 1.0, 'uniform'),\n",
    "        'colsample_bylevel': (0.01, 1.0, 'uniform'),\n",
    "        'reg_lambda': (1e-9, 1000, 'log-uniform'),\n",
    "        'reg_alpha': (1e-9, 1.0, 'log-uniform'),\n",
    "        'gamma': (1e-9, 0.5, 'log-uniform'),\n",
    "        'min_child_weight': (0, 5),\n",
    "        'n_estimators': (50, 100),\n",
    "        'scale_pos_weight': (1e-6, 500, 'log-uniform')\n",
    "    },    \n",
    "    scoring = 'roc_auc',\n",
    "    cv = StratifiedKFold(\n",
    "        n_splits=3,\n",
    "        shuffle=True,\n",
    "        random_state=42\n",
    "    ),\n",
    "    n_jobs = 3,\n",
    "    n_iter = ITERATIONS,   \n",
    "    verbose = 0,\n",
    "    refit = True,\n",
    "    random_state = 42\n",
    ")\n",
    "\n",
    "def status_print(optim_result):\n",
    "    \"\"\"Status callback durring bayesian hyperparameter search\"\"\"\n",
    "    \n",
    "    # Get all the models tested so far in DataFrame format\n",
    "    all_models = pd.DataFrame(bayes_cv_tuner.cv_results_)    \n",
    "    \n",
    "    # Get current parameters and the best parameters    \n",
    "    best_params = pd.Series(bayes_cv_tuner.best_params_)\n",
    "    print('Model #{}\\nBest ROC-AUC: {}\\nBest params: {}\\n'.format(\n",
    "        len(all_models),\n",
    "        np.round(bayes_cv_tuner.best_score_, 4),\n",
    "        bayes_cv_tuner.best_params_\n",
    "    ))\n",
    "    \n",
    "    # Save all model results\n",
    "    clf_name = bayes_cv_tuner.estimator.__class__.__name__\n",
    "    all_models.to_csv(clf_name+\"_cv_results.csv\")\n",
    "\n",
    "result_xgb = bayes_cv_tuner.fit(trains, np.array(train_y), callback=status_print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\iramaniparth\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "tests0=np.array(tests)\n",
    "ynew = result_xgb.predict_proba(tests0)\n",
    "\n",
    "d=pd.DataFrame()\n",
    "d['application_key']=key\n",
    "d['0']=ynew[:,0]\n",
    "d['1']=ynew[:,1]\n",
    "\n",
    "y=result_xgb.predict(tests0)\n",
    "\n",
    "d['pred']=y\n",
    "\n",
    "d['max']=d[['0', '1']].max(axis=1)\n",
    "\n",
    "d['max']=d['max'].apply(lambda x: round(x, 2))\n",
    "\n",
    "d['max2']=d['max']-0.001*d['pred']\n",
    "d = d.sort_values(by = ['max2'], ascending = False)\n",
    "\n",
    "sub1 = d[['application_key', 'pred']]\n",
    "\n",
    "sub1.to_csv('The_miner_league_IITGuwahati_113.csv', header = False, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model #1\n",
      "Best ROC-AUC: 0.7925\n",
      "Best params: {'colsample_bytree': 0.4160029192647807, 'learning_rate': 0.28539836866041823, 'max_bin': 940, 'max_depth': 16, 'min_child_samples': 34, 'min_child_weight': 4, 'n_estimators': 68, 'num_leaves': 74, 'reg_alpha': 5.497557739289786e-07, 'reg_lambda': 0.05936070635912049, 'scale_pos_weight': 0.060830282487222144, 'subsample': 0.13556548021189216, 'subsample_for_bin': 171234, 'subsample_freq': 6}\n",
      "\n",
      "Model #2\n",
      "Best ROC-AUC: 0.8045\n",
      "Best params: {'colsample_bytree': 0.8390144719977516, 'learning_rate': 0.5842928269761146, 'max_bin': 373, 'max_depth': 48, 'min_child_samples': 43, 'min_child_weight': 1, 'n_estimators': 57, 'num_leaves': 36, 'reg_alpha': 0.0005266983003701547, 'reg_lambda': 276.5424475574225, 'scale_pos_weight': 0.3016410771843142, 'subsample': 0.9923710598637134, 'subsample_for_bin': 406716, 'subsample_freq': 4}\n",
      "\n",
      "Model #3\n",
      "Best ROC-AUC: 0.8045\n",
      "Best params: {'colsample_bytree': 0.8390144719977516, 'learning_rate': 0.5842928269761146, 'max_bin': 373, 'max_depth': 48, 'min_child_samples': 43, 'min_child_weight': 1, 'n_estimators': 57, 'num_leaves': 36, 'reg_alpha': 0.0005266983003701547, 'reg_lambda': 276.5424475574225, 'scale_pos_weight': 0.3016410771843142, 'subsample': 0.9923710598637134, 'subsample_for_bin': 406716, 'subsample_freq': 4}\n",
      "\n",
      "Model #4\n",
      "Best ROC-AUC: 0.8045\n",
      "Best params: {'colsample_bytree': 0.8390144719977516, 'learning_rate': 0.5842928269761146, 'max_bin': 373, 'max_depth': 48, 'min_child_samples': 43, 'min_child_weight': 1, 'n_estimators': 57, 'num_leaves': 36, 'reg_alpha': 0.0005266983003701547, 'reg_lambda': 276.5424475574225, 'scale_pos_weight': 0.3016410771843142, 'subsample': 0.9923710598637134, 'subsample_for_bin': 406716, 'subsample_freq': 4}\n",
      "\n",
      "Model #5\n",
      "Best ROC-AUC: 0.8045\n",
      "Best params: {'colsample_bytree': 0.8390144719977516, 'learning_rate': 0.5842928269761146, 'max_bin': 373, 'max_depth': 48, 'min_child_samples': 43, 'min_child_weight': 1, 'n_estimators': 57, 'num_leaves': 36, 'reg_alpha': 0.0005266983003701547, 'reg_lambda': 276.5424475574225, 'scale_pos_weight': 0.3016410771843142, 'subsample': 0.9923710598637134, 'subsample_for_bin': 406716, 'subsample_freq': 4}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classifier\n",
    "bayes_cv_tuner = BayesSearchCV(\n",
    "    estimator = lgb.LGBMRegressor(\n",
    "        objective='binary',\n",
    "        metric='auc',\n",
    "        n_jobs=1,\n",
    "        verbose=0,\n",
    "        use_missing=True\n",
    "    ),\n",
    "    search_spaces = {\n",
    "        'learning_rate': (0.01, 1.0, 'log-uniform'),\n",
    "        'num_leaves': (1, 100),      \n",
    "        'max_depth': (0, 50),\n",
    "        'min_child_samples': (0, 50),\n",
    "        'max_bin': (100, 1000),\n",
    "        'subsample': (0.01, 1.0, 'uniform'),\n",
    "        'subsample_freq': (0, 10),\n",
    "        'colsample_bytree': (0.01, 1.0, 'uniform'),\n",
    "        'min_child_weight': (0, 10),\n",
    "        'subsample_for_bin': (100000, 500000),\n",
    "        'reg_lambda': (1e-9, 1000, 'log-uniform'),\n",
    "        'reg_alpha': (1e-9, 1.0, 'log-uniform'),\n",
    "        'scale_pos_weight': (1e-6, 500, 'log-uniform'),\n",
    "        'n_estimators': (50, 100),\n",
    "    },    \n",
    "    scoring = 'roc_auc',\n",
    "    cv = StratifiedKFold(\n",
    "        n_splits=3,\n",
    "        shuffle=True,\n",
    "        random_state=42\n",
    "    ),\n",
    "    n_jobs = 3,\n",
    "    n_iter = ITERATIONS,   \n",
    "    verbose = 0,\n",
    "    refit = True,\n",
    "    random_state = 42\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "result_lgb = bayes_cv_tuner.fit(trains, np.array(train_y), callback=status_print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tests0=np.array(tests)\n",
    "ynew = result_lgb.predict(tests0)\n",
    "\n",
    "d=pd.DataFrame()\n",
    "d['application_key']=key\n",
    "d['1']=pd.DataFrame(ynew)\n",
    "d['0']=1-d['1']\n",
    "\n",
    "d.loc[d['1']>d['0'], 'pred'] = 1\n",
    "d.loc[d['0']>d['1'], 'pred'] = 0\n",
    "\n",
    "#y=result_lgb.predict(test0)\n",
    "\n",
    "#d['pred']=y\n",
    "\n",
    "d['max']=d[['0', '1']].max(axis=1)\n",
    "\n",
    "d['max']=d['max'].apply(lambda x: round(x, 2))\n",
    "\n",
    "d['max2']=d['max']-0.001*d['pred']\n",
    "d = d.sort_values(by = ['max2'], ascending = False)\n",
    "\n",
    "sub1 = d[['application_key', 'pred']]\n",
    "\n",
    "sub1.to_csv('The_miner_league_IITGuwahati_114.csv', header = False, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model #1\n",
      "Best ROC-AUC: 0.8042\n",
      "Best params: {'learning_rate': 0.0028881766539144717, 'max_depth': 36, 'max_features': 8, 'min_samples_leaf': 32, 'min_samples_split': 68, 'n_estimators': 71, 'subsample': 0.35742202155015257}\n",
      "\n",
      "Model #2\n",
      "Best ROC-AUC: 0.8042\n",
      "Best params: {'learning_rate': 0.0028881766539144717, 'max_depth': 36, 'max_features': 8, 'min_samples_leaf': 32, 'min_samples_split': 68, 'n_estimators': 71, 'subsample': 0.35742202155015257}\n",
      "\n",
      "Model #3\n",
      "Best ROC-AUC: 0.8042\n",
      "Best params: {'learning_rate': 0.0028881766539144717, 'max_depth': 36, 'max_features': 8, 'min_samples_leaf': 32, 'min_samples_split': 68, 'n_estimators': 71, 'subsample': 0.35742202155015257}\n",
      "\n",
      "Model #4\n",
      "Best ROC-AUC: 0.8042\n",
      "Best params: {'learning_rate': 0.0028881766539144717, 'max_depth': 36, 'max_features': 8, 'min_samples_leaf': 32, 'min_samples_split': 68, 'n_estimators': 71, 'subsample': 0.35742202155015257}\n",
      "\n",
      "Model #5\n",
      "Best ROC-AUC: 0.8042\n",
      "Best params: {'learning_rate': 0.0028881766539144717, 'max_depth': 36, 'max_features': 8, 'min_samples_leaf': 32, 'min_samples_split': 68, 'n_estimators': 71, 'subsample': 0.35742202155015257}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classifier\n",
    "bayes_cv_tuner = BayesSearchCV(\n",
    "    estimator = GradientBoostingClassifier(),\n",
    "    search_spaces = {\n",
    "        'learning_rate': (10**-5, 10, 'log-uniform'),     \n",
    "        'max_depth': (0, 50),\n",
    "        'subsample': (0.01, 1.0, 'uniform'),\n",
    "        'n_estimators': (50, 100),\n",
    "        'min_samples_leaf': (1, 100),\n",
    "        'min_samples_split': (2, 100),\n",
    "        'max_features': (1, 8),\n",
    "    },    \n",
    "    scoring = 'roc_auc',\n",
    "    cv = StratifiedKFold(\n",
    "        n_splits=3,\n",
    "        shuffle=True,\n",
    "        random_state=42\n",
    "    ),\n",
    "    n_jobs = 3,\n",
    "    n_iter = ITERATIONS,   \n",
    "    verbose = 0,\n",
    "    refit = True,\n",
    "    random_state = 42\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "result_gbm = bayes_cv_tuner.fit(trains, np.array(train_y), callback=status_print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests0=np.array(tests)\n",
    "ynew = result_gbm.predict_proba(tests0)\n",
    "\n",
    "d=pd.DataFrame()\n",
    "d['application_key']=key\n",
    "d['0']=ynew[:,0]\n",
    "d['1']=ynew[:,1]\n",
    "\n",
    "y=result_gbm.predict(tests0)\n",
    "\n",
    "d['pred']=y\n",
    "\n",
    "d['max']=d[['0', '1']].max(axis=1)\n",
    "\n",
    "d['max']=d['max'].apply(lambda x: round(x, 2))\n",
    "\n",
    "d['max2']=d['max']-0.001*d['pred']\n",
    "d = d.sort_values(by = ['max2'], ascending = False)\n",
    "\n",
    "sub1 = d[['application_key', 'pred']]\n",
    "\n",
    "sub1.to_csv('The_miner_league_IITGuwahati_115.csv', header = False, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(train, test):\n",
    "    ntrain = train.shape[0]\n",
    "    ntest = test.shape[0]\n",
    "    train_test = pd.concat((train, test)).reset_index(drop=True)\n",
    "    numeric_feats = train_test.dtypes[train_test.dtypes != \"object\"].index\n",
    "\n",
    "    # compute skew and do Box-Cox transformation\n",
    "    skewed_feats = train[numeric_feats].apply(lambda x: skew(x.dropna()))\n",
    "    print(\"\\nSkew in numeric features:\")\n",
    "    print(skewed_feats)\n",
    "    # transform features with skew > 0.25 (this can be varied to find optimal value)\n",
    "    skewed_feats = skewed_feats[skewed_feats > 0.25]\n",
    "    skewed_feats = skewed_feats.index\n",
    "    for feats in skewed_feats:\n",
    "        train_test[feats] = train_test[feats] + 1\n",
    "        train_test[feats], lam = boxcox(train_test[feats])\n",
    "    features = train.columns\n",
    "    cats = [feat for feat in features if 'cat' in feat]\n",
    "    # factorize categorical features\n",
    "    for feat in cats:\n",
    "        train_test[feat] = pd.factorize(train_test[feat], sort=True)[0]\n",
    "    x_train = train_test.iloc[:ntrain, :]\n",
    "    x_test = train_test.iloc[ntrain:, :]\n",
    "    train_test_scaled, scaler = scale_data(train_test)\n",
    "    train, _ = scale_data(x_train, scaler)\n",
    "    test, _ = scale_data(x_test, scaler)\n",
    "\n",
    "\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Skew in numeric features:\n",
      "mvar1       0.118939\n",
      "mvar2       4.515416\n",
      "mvar3       4.220594\n",
      "mvar4       8.280500\n",
      "mvar5      10.197799\n",
      "mvar6       7.496327\n",
      "mvar7      33.476509\n",
      "mvar8       4.536171\n",
      "mvar9       8.590508\n",
      "mvar10     15.454673\n",
      "mvar11     15.572524\n",
      "mvar12      6.149522\n",
      "mvar13     51.465922\n",
      "mvar14    105.896380\n",
      "mvar15      7.825176\n",
      "mvar18      5.151784\n",
      "mvar20      3.142167\n",
      "mvar21      1.623566\n",
      "mvar22      1.012030\n",
      "mvar23      2.242447\n",
      "mvar24     41.689452\n",
      "mvar25      1.752506\n",
      "mvar26      1.575906\n",
      "mvar27      1.582929\n",
      "mvar28      0.984432\n",
      "mvar29      0.936213\n",
      "mvar30      0.055905\n",
      "mvar31      1.409096\n",
      "mvar32      2.507065\n",
      "mvar33      1.580553\n",
      "mvar34      5.941473\n",
      "mvar35      5.393156\n",
      "mvar36      2.665494\n",
      "mvar37      2.091497\n",
      "mvar38      2.756444\n",
      "mvar39     11.595155\n",
      "mvar40      0.023986\n",
      "mvar41     -1.718782\n",
      "mvar42      1.193575\n",
      "mvar43      1.652678\n",
      "mvar44     -0.464856\n",
      "mvar45      7.290179\n",
      "mvar46      8.018833\n",
      "mvar47     -0.619773\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "trains, tests = load_data(train, test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
